\begin{savequote}[0.55\linewidth]
	\begin{fancyquote}
		Each discovery made by an investigator in a basic research
		laboratory has much larger implications today. The sum of the work in basic
		biology represents a rapidly expanding tool kit for engineers and inventors to
		use to construct items of value to society.
	\end{fancyquote}
	\qauthor{David Baltimore in \emph{How Biology Became an Information Science}, 2001~\autocite{Baltimore:2001}}
\end{savequote}
\chapter{Introduction}\label{ch:introduction}

% TODO add some more intro about neuron reconstruction and why it
% is important

This thesis aims to detail the design and reimplementation of a
neuron morphology reconstruction system. This system is the result of
converting the existing \gls{orion3}~\autocite{ORION_Santamaria-Pang2015} system from
MATLAB~\autocite{MATLAB:2013a} code to native C/C++ code.
This conversion process requires an analysis of the existing code
to understand its structure and create a plan for replicating the
same functionality in the in the new system.

This software project can be categorized as a \emph{rewrite}; that
is, a replication of an existing software system without
reusing the existing code. In software engineering, the consensus
on rewriting software from scratch is that it is difficult and
that teams should avoid rewrites~\autocite{Software-rewrites:Spolsky:2000}. This view
arises because there are several challenges and risks associated
with rewriting large systems.
% reasons why rewrites fail
\begin{itemize*}[label={}]
\item Rewrites often take a long time and instead of adding
	features, development time is spent on redesign and
	reimplementation of old features.
\item In addition, all the institutional knowledge that came from
	years of bug fixes is often lost with a rewrite.
\item Rewrites are often expensive in terms of time and effort and
	are not known to payoff as much as product owners wish.
\end{itemize*}
% refactoring over rewriting
Therefore, it is preferable to work on slowly refactoring the code
rather than a complete rewrite. Refactoring is a process where
instead of throwing away the existing system, the development
proceeds by making small, incremental changes over an extended
period in order to avoid getting the software into a broken state,
while steadily improving the readability and reusability of the
software.

\section{Motivation}\label{sec:motivation}
%  [ Discussion of the application area and context of the
%    project. ]

% understand the goals of the project by talking about the background
In order to understand why this project is being undertaken, it is
important to understand why a rewrite is necessary as opposed to refactoring
the existing codebase. Both options require an analysis of the project
outcomes and deliverables, that is, a concrete set of goals that will direct the project
development. These goals can be classified as either scientific
outcomes or an engineering deliverables. Scientific outcomes are motivated by goals that
advance the state of scientific knowledge while engineering deliverables focus
on specific technical aspects of the project that make future project
maintenance and growth possible. This section will only cover the scientific
outcomes while engineering deliverables are covered in \cref{sec:objectives}.

The \emph{primary scientific outcome} of this thesis is a system for
neuron reconstruction. This system is designed with the principles of open
science in mind so that it is usable by biologists to study
neuroanatomy. \Cref{subsec:open-science} contains further
discussion on open-science and the challenge of
reproducibility that needs to be addressed by computational science projects
such as this when trying to achieve the goal of open-science.

The \emph{secondary scientific outcome} is to make this system
compatible with existing tools for image analysis so that it can
be compared against other methods as part of the BigNeuron
project. The role of BigNeuron in neuroscience research is covered
in \cref{subsec:bigneuron}.

The following sections contain an overview of the context of this project
first within the general context of scientific software (\cref{subsec:sci-soft})
and finally narrow down its role relative to the specific context
of neuron reconstruction (\cref{subsec:neuron-tracing}).

\subsection{Scientific software}\label{subsec:sci-soft}
{ % how software has become important in experimental science
	Quantitative methods are an essential part of scientific research. The
	experimental sciences depend on the dissemination of the methods used for each
	study so that it is clear how results are obtained and analyzed.
	With the rapid increase in computing power, storage, and availability,
	it has become easier to collect and process larger and more complex datasets
	using sophisticated methods and this has made software and
	software development an important part of all experimental
	fields~\autocite{Baxter2006,SSI:hettrick_2014_14809}.
	To produce reproducible results, some common approaches to this change are
	to publish either a description of the algorithm or refer to software
	that can be obtained separately (either in binary or source code form).
}

Despite these approaches, there are still several challenges to successfully
reproducing a published method.
{ % problems with a textual description
	A textual description of an algorithm is rarely a complete description of
	how an algorithm is implemented. Sophisticated methods often have tiny details
	that are mistakenly left out which may be essential for the rest of the
	processing. One specific area where this occurs often is
	in data preprocessing and annotation stages which can
	involve human interaction; as a result of this
	interaction, some assumptions
	may not be written down. For example, if the data contains
	missing values, these instances may be removed based on
	some criteria. If these criteria are not clearly written
	down, it can be difficult to apply the same procedure
	again. This is why many statisticians recommend that the
	raw data and the tidy data should both be available and if
	possible, manual processing should be avoided in the data
	preprocessing step so that it is clear how to regenerate the
	tidy data from the raw data~\autocite{Sandve2013,datasharing:Leek,Jaffe2015,Wickham:tidy-data}.
}
{ % problems with software distribution
	Referencing readily available software packages gives
	other researchers direct access to the original method
	used. However, even here, there can be problems.
	Even before running the software, it is important to
	ensure that the
	software is available years later. This means not only the software itself, but
	all its dependencies. This can quickly become complicated as technology
	advances: changes in the \acrfull{API} or \acrfull{ABI}
	can cause incompatibility issues for both software
	distributed in source form or binary form. Both source
	code and binary software can have dependencies on
	platforms (e.g., operating systems, runtime systems, and computer
	architecture) and licensing of components that can
	hinder others from using the software.
}

{ % backwards compatibility and digital preservation
	Even more precarious is when a dependency used by the
	software no longer behaves the same way as it did in
	previous versions. This can lead to software that appears
	to run, but gives unintended results. Maintaining
	backwards compatibility for software is difficult since
	there are many parts to a non-trivial software system.
	It may be possible to identify these compatibility problems using
	testing (for more discussion, see \cref{ch:testing}).
	% Dependency hell
	Resolving the exact versions of libraries and toolchains
	needed to build and run can be frustrating and is commonly
	known as \emph{dependency hell}~\autocite{anderson2000end,ModellingSoftDep:Burrows,Guo2011}.
	This problem can become
	daunting when dealing with multiple platforms and many
	libraries. As newer versions of these libraries are
	released, the maintenance phase of the \emph{systems
	development life cycle} becomes more important (further discussion in \cref{sec:sdlc}).
	Unmaintained software is prone to what is known as
	\emph{bit rot} --- that is, the process through which
	software that was working no longer works due to changes
	in the surrounding software ecosystem.
	There has been some work to prevent bit rot by recording a
	static copy of the software environment, but digital
	preservation (comprising both computing machinery and
	software) is in its infancy and has not caught up to that
	of paper-based materials~\autocite{PreservingExe2013,Thain2015,Meng2015}.
}


\subsection{Open science and scientific software engineering}\label{subsec:open-science}

As science becomes more oriented towards using computational
tools, the ideals of reproducibility and statistical hypothesis
testing become more difficult to achieve. In recent years, there
has been a push by researchers to incorporate \emph{open science}
practices in their work. One prominent advocate for open science
defines this as follows:\vspace{-3.5\parskip}
\begin{quote}
	\begin{fancyquote}
	Open science is the idea that scientific knowledge of all kinds
	should be openly shared as early as is practical in the discovery
	process.
	\end{fancyquote}
	\qauthor{Michael Nielsen~\autocite{Nielsen:open-science-def}}
\end{quote}

% add more info about quote
The ``scientific knowledge'' mentioned in the above quote
encompasses any kind of knowledge including problem definitions,
ideas, code, data, methodology, and journal articles. The goal is
to allow for more opportunities for collaboration and sharing of
information by having this information available early enough so
that it is useful to others. This can prevent expensive
duplication of effort and encourage the direct comparison of
results that is necessary for meta-analysis studies.

% CS and software; how much code is used even outside CS
For computer science research, since the research
products are often not just papers nor data, but software
implementations of algorithms, direct comparisons in terms of
metrics such as accuracy and speed must be performed on a variety of
datasets and machines. Computer science is not the only field that
makes heavy use of software development:
according to a survey of UK researchers by the Software
Sustainability Institute which spanned disciplines as diverse as
social science, medicine, and engineering, as high as 56\% of
researchers implement their own research software, but 21\% of
this subset had no training in software
development~\autocite{SSI:hettrick_2014_14809}.

% scientific code peer review & how retractions to papers can
% occur due to software errors
Since software development is already a large part of the research
process, it can be argued that a review process of scientific
software is just as important as the paper peer review process.
There already issues with papers having enough statistical power
to present reproducible results, but these problems can be
understood through careful reading of the experimental
methods~\autocite{Ioannidis2005,Button2013}. Problems in the software
are harder to analyze based on the papers alone and these have
been known to result in paper retractions~\autocite{Miller2006,Merali2010,Joppa2013}.

% how software engineering needs to be
% applied to scientific code
This lack of training in software engineering has prompted many of
the open science advocates to offer open access teaching
materials so that training is available to everyone.
In particular, the Software Carpentry organization has
published several guidelines and workshops to teach best
practices for handling software and data that are relevant to
scientific research~\autocite{Wilson2006,Wilson2014}. These
include using traditional software engineering tools and practices
such as version control systems, build automation, issue
trackers, and pre-merge code reviews.

\subsection{Open neuroscience}
% neuroscience uses quantitative imaging;
% problems with comparing data
The history of biology has been defined by the tools used to
visualize various biological structures and processes across many
scales~\cite{moore2012visualizing}. Neuroscience in particular has
made use of many techniques: from the patch clamp for studying ion
channels to \acrshort{EEG} for recording brain surface electrical
activity to \acrshort{fMRI} and fluorescent microscopy for
observing brain and neuron activity respectively, these techniques
have collectively allowed neuroscientists to characterize what
they are looking at.  These techniques have made quantitative
image analysis a large part of biology when measuring and
comparing results across different samples. However, the results
taken from different laboratories may not be comparable due to
variability in laboratory protocols~\autocite{NeuroMorphVariability:Parekh:2015}
or simply due to insufficient access to raw data necessary
necessary to accurately use Bayesian inference for
predictions~\autocite{Poldrack2011}.

To address this problem, projects such as the \acrfull{NIF}~\cite{Gardner2008}
and \acrfull{NITRC}~\cite{Kennedy2016} provide databases of neuroscience data
and tools that are machine-readable. These are part of recent research efforts
to improve brain mapping and modelling~\autocite{Markram2013,BigBrainIEEESpectrum:2013}.
Some of these efforts are based on manual data annotation~\autocite{Ascoli2007,Helmstaedter2011,Helmstaedter2012,Marx2013},
while others such as the BigNeuron project from the Allen Institute for
Brain Science are focused on completely automated reconstruction. This last
project is the subject of \cref{subsec:bigneuron}.

\subsection{BigNeuron}\label{subsec:bigneuron}

% BigNeuron and what is Vaa3D
The secondary scientific outcome is to provide an implementation of
\gls{orion3} for the BigNeuron project. This requires using the common
interface provided by the the Vaa3D biomedical imaging
toolkit~\autocite{Vaa3D:site:2015,Vaa3D:Peng:2010,Vaa3D:Peng:2014}.  This
toolkit allows biologists to visualize and analyze biomedical imaging datasets.
Vaa3D can be extended through the development of plugins. This allows algorithm
developers to make their interactive and non-interactive methods for image
analysis available for biologists to use without having to switch between
multiple programs for processing and visualizing data.

% explain comparing algorithms and DIADEM Challenge
Creating an image analysis tool that can be easily integrated into
other systems allows others to reproduce the results in order to
compare with both ground truth data and other algorithms. There
has been an attempt at comparing neuron morphology extraction
algorithms in the past under the \acrshort{DIADEM} Challenge which
contributed datasets and a metric for comparing the neuron
tracings from algorithms against tracings from gold standard
segmentations~\autocite{DIADEM&Beyond:Liu:2011,DIADEM-dataset:Brown:2011,DIADEM-metric-Gillette2011}.
The DIADEM Challenge used six datasets from different neuroscience
institutions in order to have a diversity in terms of source of
the neurons (i.e., from different species and structures from
different brain regions) and in terms of the laboratory protocols
(i.e., different labelling and microscopy techniques).

The DIADEM Challenge was successful in raising awareness of the
problem of neuron reconstruction and several new 3D reconstruction
methods and metrics have been proposed after the end of the DIADEM
Challenge (see \cref{subsec:neuron-tracing}).  However, the
lack of larger public datasets, standardized metrics, and readily
available algorithms have made it difficult to compare new methods
for neuron reconstruction.

% explain BigNeuron
To solve this problem, an open-science project called
BigNeuron~\autocite{BigNeuron:Peng:2015,DIADEM2BigNeuron:Peng:2015}
continues where the DIADEM Challenge left off. Instead of
comparing the output on a few datasets as in the DIADEM Challenge,
BigNeuron aims to enable high-throughput analysis of neuron
microscopy stacks using multiple methods contributed from various
algorithm developers. By standardizing on the Vaa3D platform, this
precludes differences between how each method handles data which
means that all the algorithms can be bench-tested at once without
a need for translating data between formats.

However, since the Vaa3D is written in C++, incorporating plugin
code written in non-native languages poses a problem. For this
reason, the BigNeuron project organizers recommend that all
algorithms that are submitted be in C or C++. From the BigNeuron
FAQ~\autocite{BigNeuron:FAQ:2015}\\
\parbox{\textwidth}{
\begin{quote}
	\begin{fancyquote}
		{Q3. \bfseries How can I incorporate code written in Matlab, Java, Python or another language other than C/C++?}

		A. BigNeuron is a very large scale project, and enforcing a
		unified API is critical to ensure fair comparison for any pre-defined
		assessment. We thus discourage usage of Matlab, Java, Python or other
		programming languages besides C/C++ for this bench
		testing.
	\end{fancyquote}
\end{quote}
}

This is why a rewrite of the code is necessary rather than trying
to integrate MATLAB via the MATLAB Engine Interface. Further
discussion of the benefits of this decision are discussed in \cref{sec:benefits}.

\section[\glsentrylong{SDLC}]{\Glsentrylong{SDLC}}\label{sec:sdlc}

Before starting with the development, we need to outline the general steps
needed to achieve the above stated outcomes and deliverables. In systems engineering,
these steps are called the \acrfull{SDLC} of the project~\autocite{IRM-SDLC:DoJ:2003}.
There are many variations of the \acrshort{SDLC} each suited to different kinds
of projects; \Cref{fig:sdlc} depicts a simple version used in this project with six phases:

\begin{description}[font=\textpluscolon]
	\item[Planning] In this phase, project management
		and resource allocation details are determined.
		This includes scheduling, tools, and
		defining project objectives;
	\item[Analysis] This phase analyzes the project
		requirements and defines specific
		technical milestones that relate to the
		objectives defined in the Planning phase;
	\item[Design] The parts of the system are
		delineated and the expected input/output
		characteristics of each part are
		determined;
	\item[Implementation] The physical system is built
		during this phase using the system design
		from the previous phase;
	\item[Testing] As the Implementation phase
		proceeds, each standalone part is tested
		individually in what are known as unit
		tests. When the parts interact with each
		other, integration tests are conducted to
		determine that these parts are compatible;
	\item[Maintenance] In this phase, the system is put in
		production and monitored for changes in system
		performance and project requirements. When these
		changes necessitate an update to the system, the
		project may return to the Planning phase.
\end{description}

It is important to note that the phases of this life cycle
are not discrete; there is overlap between phases. For
example, parts of the design may change as the
implementation continues as more information about the
physical system is available.

\begin{figure}[tbp]
\centering
\input{gfx/sdlc.tex}
\caption[Systems development life cycle]%
	{\textbf{A simple systems development life cycle (SDLC)}: This figure
	depicts an example of a life cycle used to delineate the
	phases of development.
}\label{fig:sdlc}
\end{figure}

\section{System objectives}\label{sec:objectives}

As opposed to the scientific outcomes discussed in \cref{sec:motivation},
the system objectives are the engineering
deliverables; these are more closely tied to the Design and Implementation
phases as these objectives influence decisions made about the underlying
resources and system architecture.

% Define the project objectives (engineering deliverables).
% These are expanded from the list given in the abstract.
The \emph{first objective} is a complete conversion of the
existing code from MATLAB to native code which will be referred to
as \gls{orionmat} and \gls{orionc} respectively. This involves an
analysis of the existing code to see which parts of the algorithm will be converted.

The \emph{second objective} is the ability to easily integrate the system with
Vaa3D as a plugin. This requires looking at the interface that
Vaa3D uses and creating compatible data structures so that the
data does not need to converted between multiple formats in
memory.

The \emph{third objective} is a test suite to verify that the
components of the \gls{orionc} system operate on the same input
and produce expected outputs which are comparable to the
\gls{orionmat} code and follow expected properties outlined in the
design.

The \emph{fourth objective} is to ensure that the system provides
a means for reproducibility by testing the software under different
conditions and making it possible to the replicate the software
environment so that others may run the software.

\section{Benefits}\label{sec:benefits}
% Analyse project requirements: why are these objectives
% beneficial?

The first objective requires that all of the MATLAB code be
replaced by native code. This provides several benefits:
\begin{enumerate}[label={\alph*)}]
	\item it removes a dependency on MATLAB which requires that
		all users either install a licensed copy of MATLAB
		or use the MATLAB Compiler Runtime for deployment;
	\item it provides the benefit that changes in
		function behavior between versions of
		MATLAB do not effect the output of the
		code; and
	\item certain operations can run faster in native code than
		in MATLAB.
\end{enumerate}

The second objective will allow the code to work with a widely used
tool for visualization and analysis. Furthermore, integrating with
one tool will provide a framework for integrating with other
biomedical image analysis tools such as ImageJ~\autocite{Schneider2012}.

The third objective provides a safety net so that current and
future development clearly defines the expectation of the code
not only in the documentation, but as executable tests that can be
used to indicate when a change causes these expectations to no
longer be met.

The fourth objective is what ensues that the previously discussed
benefits are available for others to use on their machines and
allowing for reproducibility of the neuron reconstruction results.
This objective is what makes the project an ``open-science''
project.

\section{Literature review}

\subsection{Neuron reconstruction and tracing}\label{subsec:neuron-tracing}

TODO: discuss neuron tracing datasets that exist
~\autocite{Duke-Southampton-archive:Cannon:1998,DIADEM-dataset:Brown:2011}

TODO: History of neuron-tracing algorithms up to DIADEM
~\autocite{DIADEM&Beyond:Liu:2011,NeuroMorphTrends:Halavi:2012,NeuroTracePerspect:Meijering:2010}

Post-DIADEM algorithms
~\autocite{Bauer2010,MIA-anisotropic-path-searching-Xie2011,MICCAI-anisotropic-path-searching-Xie2010,
Jeong2015,Luo2015,De2015,Gulyanon2015,ORION_Santamaria-Pang2015,Mukherjee2014,Hernandez-Herrera2014,Basu2014,Xiao2013,Jimenez2013,Basu2013,Mukherjee2013,Hernandez-Herrera2013,Ming2013,Lee2012,Czarnecki2012}

TODO: post-DIADEM metrics~\autocite{Mayerich2011,Mayerich2012,btmorph-Torben-Nielsen2014,Costa2014,Gillette2015}

\begin{enumerate}[label=(\roman*)]
\item to evaluate the MATLAB codebase of \gls{orion3} and determine how to structure the new codebase
\item integrate the
\end{enumerate}

% TODO
%
% - refactoring and conversion of code
% - Open science
% - bioinformatics
