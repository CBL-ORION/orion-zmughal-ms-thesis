\begin{savequote}[0.55\linewidth]
	\begin{fancyquote}
		Each discovery made by an investigator in a basic research
		laboratory has much larger implications today. The sum of the work in basic
		biology represents a rapidly expanding tool kit for engineers and inventors to
		use to construct items of value to society.
	\end{fancyquote}
	\qauthor{David Baltimore in \emph{How Biology Became an Information Science}, 2001~\autocite{Baltimore:2001}}
\end{savequote}
\chapter{Introduction}\label{ch:introduction}

% TODO add some more intro about neuron reconstruction and why it
% is important

This thesis aims to detail the design and reimplementation of a
neuron morphology reconstruction system. This system is the result of
converting the existing \gls{orion3}~\autocite{ORION_Santamaria-Pang2015} system from
MATLAB~\autocite{MATLAB:2013a} code to native C/C++ code.
This conversion process requires an analysis of the existing code
to understand its structure and create a plan for replicating the
same functionality in the in the new system.

This software project can be categorized as a \emph{rewrite}; that
is, a replication of an existing software system without
reusing the existing code. In software engineering, the consensus
on rewriting software from scratch is that it is difficult and
that teams should avoid rewrites~\autocite{Software-rewrites:Spolsky:2000}. This view
arises because there are several challenges and risks associated
with rewriting large systems.
% reasons why rewrites fail
\begin{itemize*}[label={}]
\item Rewrites often take a long time and instead of adding
	features, development time is spent on redesign and
	reimplementation of old features.
\item In addition, all the institutional knowledge that came from
	years of bug fixes is often lost with a rewrite.
\item Rewrites are often expensive in terms of time and effort and
	are not known to pay off as much as product owners wish.
\end{itemize*}
% refactoring over rewriting
Therefore, it is preferable to work on slowly refactoring the code
rather than a complete rewrite. Refactoring is a process where
instead of throwing away the existing system, the development
proceeds by making small, incremental changes over an extended
period in order to avoid getting the software into a broken state,
while steadily improving the maintainability and reusability of the
software.

\section{Motivation}\label{sec:motivation}
%  [ Discussion of the application area and context of the
%    project. ]

% understand the goals of the project by talking about the background
In order to understand why this project is being undertaken, it is
important to understand why a rewrite is necessary as opposed to refactoring
the existing codebase. Both options require an analysis of the project
outcomes and deliverables, that is, a concrete set of goals that will direct the project
development. These goals can be classified as either scientific
outcomes or engineering deliverables. Scientific outcomes are motivated by goals that
advance the state of scientific knowledge while engineering deliverables focus
on specific technical aspects of the project that make future project
maintenance and growth possible. This section will only cover the scientific
outcomes while engineering deliverables are covered in \cref{sec:objectives}.

The \emph{primary scientific outcome} of this thesis is a system for
neuron reconstruction. This system is designed with the principles of open
science in mind so that it is usable by biologists to study
neuroanatomy. \Cref{subsec:open-science} contains further
discussion on open-science and the challenge of
reproducibility that needs to be addressed by computational science projects
such as this when trying to achieve the goal of open-science.

The \emph{secondary scientific outcome} is to make this system
compatible with existing tools for image analysis so that it can
be compared against other methods as part of the BigNeuron
project. The role of BigNeuron in neuroscience research is covered
in \cref{subsec:bigneuron}.

The following sections contain an overview of the context of this project
first within the general context of scientific software (\cref{subsec:sci-soft})
and finally narrow down its role relative to the specific context
of neuron reconstruction (\cref{sec:neuron-tracing}).

\subsection{Scientific software}\label{subsec:sci-soft}
{ % how software has become important in experimental science
	Quantitative methods are an essential part of scientific research. The
	experimental sciences depend on the dissemination of the methods used for each
	study so that it is clear how results are obtained and analyzed.
	With the rapid increase in computing power, storage, and availability,
	it has become easier to collect and process larger and more complex datasets
	using sophisticated methods and this has made software and
	software development an important part of all experimental
	fields~\autocite{Baxter2006,SSI:hettrick_2014_14809}.
	To produce reproducible results, some common approaches to this change are
	to publish either a description of the algorithm or refer to software
	that can be obtained separately (either in binary or source code form).
}

Despite these approaches, there are still several challenges to successfully
reproducing a published method.
{ % problems with a textual description
	A textual description of an algorithm is rarely a complete description of
	how an algorithm is implemented. Sophisticated methods often have tiny details
	that are mistakenly left out which may be essential for the rest of the
	processing. One specific area where this occurs often is
	in data preprocessing and annotation stages which can
	involve human interaction; as a result of this
	interaction, some assumptions
	may not be written down. For example, if the data contain
	instances with missing values, these instances may be removed based on
	some criteria. If these criteria are not clearly written
	down, it can be difficult to apply the same procedure
	again. This is why many statisticians recommend that the
	raw data and the tidy data should both be available and if
	possible, manual processing should be avoided in the data
	preprocessing step so that it is clear how to regenerate the
	tidy data from the raw data~\autocite{Sandve2013,datasharing:Leek,Jaffe2015,Wickham:tidy-data}.
}
{ % problems with software distribution
	Referencing readily available software packages gives
	other researchers direct access to the original method
	used. However, even here, there can be problems.
	Even before running the software, it is important to
	ensure that the
	software is available years later. This means not only the software itself, but
	all its dependencies. This can quickly become complicated as technology
	advances: changes in the \acrfull{API} or \acrfull{ABI}
	can cause incompatibility issues for both software
	distributed in source form or binary form. Both source
	code and binary software can have dependencies on
	platforms (e.g., operating systems, runtime systems, and computer
	architecture) and licensing of components that can
	hinder others from using the software.
}

{ % backwards compatibility and digital preservation
	Even more precarious is when a dependency used by the
	software no longer behaves the same way as it did in
	previous versions. This can lead to software that appears
	to run, but gives unintended results. Maintaining
	backwards compatibility for software is difficult since
	there are many parts to a non-trivial software system.
	It may be possible to identify these compatibility problems using
	testing (for more discussion, see \cref{ch:testing}).
	% Dependency hell
	Resolving the exact versions of libraries and toolchains
	needed to build and run can be frustrating and is commonly
	known as \emph{dependency hell}~\autocite{anderson2000end,ModellingSoftDep:Burrows,Guo2011}.
	This problem can become
	daunting when dealing with multiple platforms and many
	libraries. As newer versions of these libraries are
	released, the maintenance phase of the \emph{systems
	development life cycle} becomes more important (further discussion in \cref{sec:sdlc}).
	Unmaintained software is prone to what is known as
	\emph{bit rot} --- that is, the process through which
	software that was working no longer works due to changes
	in the surrounding software ecosystem.
	There has been some work to prevent bit rot by recording a
	static copy of the software environment, but digital
	preservation (comprising both computing machinery and
	software) is in its infancy and has not caught up to that
	of paper-based materials~\autocite{PreservingExe2013,Thain2015,Meng2015}.
}


\subsection{Open science and scientific software engineering}\label{subsec:open-science}

As science becomes more oriented towards using computational
tools, the ideals of reproducibility and statistical hypothesis
testing become more difficult to achieve. In recent years, there
has been a push by researchers to incorporate \emph{open science}
practices in their work. One prominent advocate for open science
defines this as follows:\vspace{-3.5\parskip}
\begin{quote}
	\begin{fancyquote}
	Open science is the idea that scientific knowledge of all kinds
	should be openly shared as early as is practical in the discovery
	process.
	\end{fancyquote}
	\qauthor{Michael Nielsen~\autocite{Nielsen:open-science-def}}
\end{quote}

% add more info about quote
The ``scientific knowledge'' mentioned in the above quote
encompasses any kind of knowledge including problem definitions,
ideas, code, data, methodology, and journal articles. The goal is
to allow for more opportunities for collaboration and sharing of
information by having this information available early enough so
that it is useful to others. This can prevent expensive
duplication of effort and encourage the direct comparison of
results that is necessary for meta-analysis studies.

% CS and software; how much code is used even outside CS
For computer science research, since the research
products are often not just papers nor data, but software
implementations of algorithms, direct comparisons in terms of
metrics such as accuracy and speed must be performed on a variety of
datasets and machines. Computer science is not the only field that
makes heavy use of software development:
according to a survey of UK researchers by the Software
Sustainability Institute which spanned disciplines as diverse as
social science, medicine, and engineering, as high as 56\% of
researchers implement their own research software, but 21\% of
this subset had no training in software
development~\autocite{SSI:hettrick_2014_14809}.

% scientific code peer review & how retractions to papers can
% occur due to software errors
Since software development is already a large part of the research
process, it can be argued that a review process of scientific
software is just as important as the paper peer review process.
There are already issues with papers not having enough statistical power
to present reproducible results, but these problems can be
understood through careful reading of the experimental
methods~\autocite{Ioannidis2005,Button2013}. Problems in the software
are harder to analyze based on the papers alone and these have
been known to result in paper retractions~\autocite{Miller2006,Merali2010,Joppa2013}.

% how software engineering needs to be
% applied to scientific code
This lack of training in software engineering has prompted many
open science advocates to offer open access teaching
materials so that training is available to everyone.
In particular, the Software Carpentry organization has
published several guidelines and workshops to teach best
practices for handling software and data that are relevant to
scientific research~\autocite{Wilson2006,Wilson2014}. These
include using traditional software engineering tools and practices
such as version control systems, build automation, issue
trackers, and pre-merge code reviews.

\subsection{Open neuroscience}
% neuroscience uses quantitative imaging;
% problems with comparing data
The history of biology has been defined by the tools used to
visualize various biological structures and processes across many
scales~\autocite{moore2012visualizing}. Neuroscience in particular has
made use of many techniques: from the patch clamp for studying ion
channels to \acrshort{EEG} for recording brain surface electrical
activity to \acrshort{fMRI} and fluorescent microscopy for
observing brain and neuron activity respectively, these techniques
have collectively allowed neuroscientists to characterize what
they are looking at.  These techniques have made quantitative
image analysis a large part of biology when measuring and
comparing results across different samples. However, the results
taken from different laboratories may not be comparable due to
variability in laboratory protocols~\autocite{NeuroMorphVariability:Parekh:2015}
or simply due to insufficient access to raw data necessary
to accurately use Bayesian inference for
predictions~\autocite{Poldrack2011}.

To address this problem, projects such as the \acrfull{NIF}~\autocite{Gardner2008}
and \acrfull{NITRC}~\autocite{Kennedy2016} provide databases of neuroscience data
and tools that are machine-readable. These are part of recent research efforts
to improve brain mapping and modelling~\autocite{Markram2013,BigBrainIEEESpectrum:2013}.
Some of these efforts are based on manual data annotation~\autocite{Ascoli2007,Helmstaedter2011,Helmstaedter2012,Marx2013},
while others such as the BigNeuron project from the Allen Institute for
Brain Science are focused on completely automated reconstruction. This last
project is the subject of \cref{subsec:bigneuron}.

\subsection{BigNeuron}\label{subsec:bigneuron}

% BigNeuron and what is Vaa3D
The secondary scientific outcome is to provide an implementation of
\gls{orion3} for the BigNeuron project. This requires using the common
interface provided by the Vaa3D biomedical imaging
toolkit~\autocite{Vaa3D:site:2015,Vaa3D:Peng:2010,Vaa3D:Peng:2014}.  This
toolkit allows biologists to visualize and analyze biomedical imaging datasets.
Vaa3D can be extended through the development of plugins. This allows algorithm
developers to make their interactive and non-interactive methods for image
analysis available for biologists to use without having to switch between
multiple programs for processing and visualizing data.

% explain comparing algorithms and DIADEM Challenge
Creating an image analysis tool that can be easily integrated into
other systems allows others to reproduce the results in order to
compare with both ground truth data and other algorithms. There
has been an attempt at comparing neuron morphology extraction
algorithms in the past under the \acrshort{DIADEM} Challenge which
contributed datasets and a metric for comparing the neuron
tracings from algorithms against tracings from gold standard
reconstructions~\autocite{DIADEM&Beyond:Liu:2011,DIADEM-dataset:Brown:2011,DIADEM-metric-Gillette2011}.
The DIADEM Challenge used six datasets from different neuroscience
institutions in order to have a diversity in terms of source of
the neurons (i.e., from different species and structures from
different brain regions) and in terms of the laboratory protocols
(i.e., different labelling and microscopy techniques).

The DIADEM Challenge was successful in raising awareness of the
problem of neuron reconstruction and several new 3D reconstruction
methods and metrics have been proposed after the end of the DIADEM
Challenge (see \cref{sec:neuron-tracing}).  However, the
lack of larger public datasets, standardized metrics, and readily
available algorithms have made it difficult to compare new methods
for neuron reconstruction.

% explain BigNeuron
To solve this problem, an open-science project called
BigNeuron~\autocite{BigNeuron:Peng:2015,DIADEM2BigNeuron:Peng:2015}
continues where the DIADEM Challenge left off. Instead of
comparing the output on a few datasets, as in the DIADEM Challenge,
BigNeuron aims to enable high-throughput analysis of neuron
microscopy stacks using multiple methods contributed from various
algorithm developers. By standardizing on the Vaa3D platform, this
precludes any issues that might arise due to differences between how each method handles data which
means that all the algorithms can be bench-tested at once without
a need for translating data between formats.

However, since the Vaa3D is written in C++, incorporating plugin
code written in non-native languages poses a problem. For this
reason, the BigNeuron project organizers recommend that all
algorithms that are submitted be in C or C++. From the BigNeuron
FAQ~\autocite{BigNeuron:FAQ:2015}\\
\parbox{\textwidth}{
\begin{quote}
	\begin{fancyquote}
		{Q3. \bfseries How can I incorporate code written in Matlab, Java, Python or another language other than C/C++?}

		A. BigNeuron is a very large scale project, and enforcing a
		unified API is critical to ensure fair comparison for any pre-defined
		assessment. We thus discourage usage of Matlab, Java, Python or other
		programming languages besides C/C++ for this bench
		testing.
	\end{fancyquote}
\end{quote}
}

This is why a rewrite of the code is necessary rather than trying
to integrate MATLAB via the MATLAB Engine Interface. Further
discussion of the benefits of this decision are discussed in \cref{sec:benefits}.

\section[\glsentrylong{SDLC}]{\Glsentrylong{SDLC}}\label{sec:sdlc}

Before starting with the development, we need to outline the general steps
needed to achieve the above stated outcomes and deliverables. In systems engineering,
these steps are called the \acrfull{SDLC} of the project~\autocite{IRM-SDLC:DoJ:2003}.
There are many variations of the \acrshort{SDLC} each suited to different kinds
of projects; \Cref{fig:sdlc} depicts a simple version used in this project with six phases:

\begin{description}[font=\textpluscolon]
	\item[Planning] In this phase, project management
		and resource allocation details are determined.
		This includes scheduling, tools, and
		defining project objectives;
	\item[Analysis] This phase analyzes the project
		requirements and defines specific
		technical milestones that relate to the
		objectives defined in the Planning phase;
	\item[Design] The parts of the system are
		delineated and the expected input/output
		characteristics of each part are
		determined;
	\item[Implementation] The physical system is built
		during this phase using the system design
		from the previous phase;
	\item[Testing] As the Implementation phase
		proceeds, each standalone part is tested
		individually in what are known as unit
		tests. When the parts interact with each
		other, integration tests are conducted to
		determine that these parts are compatible;
	\item[Maintenance] In this phase, the system is put in
		production and monitored for changes in system
		performance and project requirements. When these
		changes necessitate an update to the system, the
		project may return to the Planning phase.
\end{description}

It is important to note that the phases of this life cycle
are not discrete; there is overlap between phases. For
example, parts of the design may change as the
implementation continues as more information about the
physical system is available.

\begin{figure}[tbp]
\centering
\input{gfx/sdlc.tex}
\caption[Systems development life cycle]%
	{\textbf{A simple systems development life cycle (SDLC)}: This figure
	depicts an example of a life cycle used to delineate the
	phases of development.
}\label{fig:sdlc}
\end{figure}

\section{System objectives}\label{sec:objectives}

As opposed to the scientific outcomes discussed in \cref{sec:motivation},
the system objectives are the engineering
deliverables; these are more closely tied to the Design and Implementation
phases as these objectives influence decisions made about the underlying
resources and system architecture.

% Define the project objectives (engineering deliverables).
% These are expanded from the list given in the abstract.
The \emph{first objective} is a complete conversion of the
existing code from MATLAB to native code which will be referred to
as \gls{orionmat} and \gls{orionc} respectively. This involves an
analysis of the existing code to see which parts of the algorithm will be converted.

The \emph{second objective} is the ability to easily integrate the system with
Vaa3D as a plugin. This requires looking at the interface that
Vaa3D uses and creating compatible data structures so that the
data does not need to converted between multiple formats in
memory.

The \emph{third objective} is a test suite to verify that the
components of the \gls{orionc} system operate on the same input
and produce expected outputs which are comparable to the
\gls{orionmat} code and follow expected properties outlined in the
design.

The \emph{fourth objective} is to ensure that the system provides
a means for reproducibility by testing the software under different
conditions and making it possible to the replicate the software
environment so that others may run the software.

\section{Benefits}\label{sec:benefits}
% Analyse project requirements: why are these objectives
% beneficial?

The first objective requires that all of the MATLAB code be
replaced by native code. This provides several benefits:
\begin{enumerate}[label={\alph*)}]
	\item it removes a dependency on MATLAB which requires that
		all users either install a licensed copy of MATLAB
		or use the MATLAB Compiler Runtime for deployment;
	\item it provides the benefit that changes in
		function behavior between versions of
		MATLAB do not effect the output of the
		code; and
	\item it implements certain operations that can run faster in native code than
		in MATLAB.
\end{enumerate}

The second objective will allow the code to work with Vaa3D, a widely used
tool for visualization and analysis. Furthermore, integrating with
one tool will provide a framework for integrating with other
biomedical image analysis tools such as ImageJ~\autocite{Schneider2012}.

The third objective provides a safety net so that current and
future development clearly defines the expectation of the code
not only in the documentation, but as executable tests that
indicate when a change causes these expectations to no
longer be met.

The fourth objective is what ensues that the previously discussed
benefits are available for others to use on their machines and
allowing for reproducibility of the neuron reconstruction results.
This objective is what makes the project an ``open-science''
project.

\section{Neuron reconstruction and tracing}\label{sec:neuron-tracing}

Computational neuromorphology was first started in the late 1960s
with initial attempts to capture microscopy images for computer
storage and analysis~\autocite{NeuroMorphTrends:Halavi:2012,NeuroTracePerspect:Meijering:2010}.
However, due to limitations in computer processing and storage, it
was not until the late 1990s that a public dataset of neuron
morphology was available~\autocite{Duke-Southampton-archive:Cannon:1998}.
With the release of the DIADEM Challenge data, there was an
opportunity for more researchers to participate in the development
of automated neuron reconstruction algorithms~\autocite{DIADEM-dataset:Brown:2011}.

After the DIADEM Challenge concluded, there have been several
papers that provide new approaches for neuron reconstruction.  The
papers can be grouped roughly into either segmentation-based
reconstruction or seed point reconstruction methods. Segmentation-based
reconstruction uses image features to determine tubular structures
for labelling the volume and as such obtains the centerline and
structure boundary at once. Seed point segmentation works by
obtaining seed points that are representative of the centerline
and then connecting these seed points to reconstruct the neuron.

\subsection{Segmentation-based reconstruction}

In \textcite{Bauer2010}, the authors present a method for
automatic segmentation of 3D volumes of blood
vessels especially where the vessel tree
contains many overlapping structures. This
is achieved by first extracting the
tubular shapes using a Hessian filter. The
results of the Hessian filter are then
further processed by using a medialness filter
to suppress false data (such as tumors or
overlapping structures) that might occur at
the boundaries of the vessel. Once the
tubular structures are detected, the
centreline is extracted by traversing the
medialness response along the path with
maximal value. The centreline and boundary provides structural
information that can be used to construct
model of the vessel structure that takes
into account properties of blood flow
(vessel radius, branching angle). This
structural information is then used as
a shape prior for assigning each tube to a
tree in order to handle multiple
trees.

In \textcite{Jimenez2013}, the authors
present a method that first uses low-pass, high-pass, and Laplacian filter responses
as features to train an \acrshort{SVM} to classify voxels as
being background or foreground. This is
used to binarize the neuron data so that
only a subset of the volume needs to be
analysed for seed selection. Seed
selection is performed by applying a
distance transform to the binary volume
and then only keeping voxels that have a
greater distances than the average
distance in the 26-neighborhood of that
voxel. Tracing between these seed points
is then done with a variation of
Dijkstra's algorithm that adds weights
based on the distance transform
and on the orientation between each successive
edge. These constraints keep the centerline path
from deviating from the center of the
tubular structure and from having sharp
changes in direction.

\Textcite{Hernandez-Herrera2014} describes
a segmentation method based on one-class
classification. This segmentation method
uses the Laplacian filter response in
order to create a training set of
background voxels. Two representative
eigenvalues of the Hessian filter are then
used to create a discriminant function
based on the distribution of these
eigenvalues. This discriminant function is then used for
segmentation.

\subsection{Seed point reconstruction}

In \textcite{MICCAI-anisotropic-path-searching-Xie2010,MIA-anisotropic-path-searching-Xie2011},
the authors cover a method for tracing
based on choosing seed points that
represent the underlying neuronal
structure and then connecting these seeds
to produce a neuron tree. The seeding is
initiated by applying a windowing cube on
the volume in order to detect seed
candidates at the local maxima. These
seeds candidates are pruned to simplify
tracing. Then a cost function is
introduced in order to a find a path
between the seed points. This cost
function incorporates a smoothness term
which constrains new edges between seed
points so that large changes in
orientation are avoided. Finally, these
local edges are used to construct a
global tree by using the
minimal-spanning-tree (MST) algorithm.

\Textcite{Luo2015} presents a method for neuron
reconstruction that is based on an
open-curve snake model. This method
start by a seed detection step that is
based on what is known as a Sliding Volume
Filter which selects a spherical region
around each voxel in order to calculate
the orientation of the gradient vectors
within that region relative to center of
the spherical region. By calculating the
filter response as the cosine of the
orientation, regions with a high response
have more voxels where the gradient is
oriented towards the center which
indicates a tubular region. This filter
response is used for initial seed point
selection. These seed points are further
pruned by using the eigenvalues of the
Hessian of the volume to choose points
that lie along the centerline of the
tubular structure. The intensity of the
voxels at the seed points is recalculated to
create an SVF-enhanced image which is then
used by an open-curve snake model for
neuron tracing. This model creates a
deformable model that is used to add edges
to a curve that extends along the
direction of the eigenvalue of the Hessian
calculated earlier.

\Textcite{Gulyanon2015}
 uses a joint probability model
to both optimize voxel labels (foreground,
background) and open-curve snake model
configurations. This probability model
uses the Frangi vessellness features as a
data prior. A Discriminative Random
Field is used to determine initial labels
for the voxel classifications and voxels
with a high confidence are used as the 
initial seed points of 3D snake configuration.
This inference is performed on subvolumes
so that the probability model only uses
the local intensity distribution.

\subsection{Neuron tree similarity metrics}

The DIADEM Challenge also provided a metric for comparing tracings
from automated reconstruction to gold standard tracings from human
experts~\autocite{DIADEM-metric-Gillette2011,Gillette2015}.
			This method works on the SWC
			format which provides both a spatial
			location and a radius for each point in the
			neuron tree. The method of registering the
			test tracing to the gold standard tracing
			starts by choosing an unregistered gold
			standard bifurcation node. To check that
			there is a matching node in the test
			tracing, the method looks for a test node
			that is in within a cylindrical region
			around the same spatial location as the
			gold standard node in order to find a
			match. The matching continues along both
			the parent and any children of the gold
			standard node. In order to determine that
			a gold standard node and test node are
			sufficiently close, a path length error is
			calculated by taking the path length
			between a nodes in the test tracing
			divided by the corresponding path length
			in the gold standard tracing. As long as
			this path length error is within
			acceptable bounds, the path is considered
			a match. Finally, if there are any gold
			standard nodes that have not been
			registered, the algorithm tries to
			determine if any existing path that passes
			through that node in order to determine a
			match.

Recently, a new method for measuring the similarity of neuron
tracings has been proposed in
\autocite{Mayerich2011,Mayerich2012}.
			This method
			measures the similarity between two
			tracings by using a Gaussian-weighted distance
			field between a given point in one tree
			and the closest point in the other
			tree. This allows for small deviations
			between the two trees without excessive
			penalization. This similarity metric is
			then applied to every point in one tree in order
			to determine a graph coloring that assigns
			each node in one tree to a corresponding
			node in the other tree.

There are still many more challenges for working with
neuromorphology data including working with time-lapse data,
handling subcellular structures and dense trees that contain
multiple neurons, and dealing with electron microscopy
data~\autocite{DIADEM&Beyond:Liu:2011}. The target of all these
challenges is to capture the dynamics of large neural circuits in
hopes that understanding how groups of neurons work will provide
insight into brain function.

